---
 Title: Publications
---
<ul class="paper-list"><li class="paper-item"><div class="papertitle"><div>Fighting the
COVID-19 infodemic with a holistic BERT ensemble</div></div><div
class="authors"><div>Tziafas Georgios, Kogkalidis Konstantinos, Caselli
Tommaso</div></div><div class="venue"><div> In Proceedings of the fourth
workshop on nlp for internet freedom: Censorship, disinformation, and
propaganda. June 2021.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/2021.nlp4if-1.18"
target="_blank"> paper </a>] [<a
href="https://github.com/gtziafas/nlp4ifchallenge" target="_blank"> code
</a>] <div class="abstract"><p>This paper describes the TOKOFOU system,
an ensemble model for misinformation detection tasks based on six
different transformer-based pre-trained encoders, implemented in the
context of the COVID-19 Infodemic Shared Task for English. We fine tune
each model on each of the task’s questions and aggregate their
prediction scores using a majority voting approach. TOKOFOU obtains an
overall F1 score of 89.7%, ranking first.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Neural proof
nets</div></div><div class="authors"><div>Kogkalidis Konstantinos,
Moortgat Michael, Moot Richard</div></div><div class="venue"><div> In
Proceedings of the 24th conference on computational natural language
learning. November 2020.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/2020.conll-1.3" target="_blank">
paper </a>] [<a
href="https://github.com/konstantinosKokos/neural-proof-nets"
target="_blank"> code </a>] <div class="abstract"><p>Linear logic and
the linear λ-calculus have a long standing tradition in the study of
natural language form and meaning. Among the proof calculi of linear
logic, proof nets are of particular interest, offering an attractive
geometric representation of derivations that is unburdened by the
bureaucratic complications of conventional prooftheoretic formats.
Building on recent advances in set-theoretic learning, we propose a
neural variant of proof nets based on Sinkhorn networks, which allows us
to translate parsing as the problem of extracting syntactic primitives
and permuting them into alignment. Our methodology induces a
batch-efficient, end-to-end differentiable architecture that actualizes
a formally grounded yet highly efficient neuro-symbolic parser. We test
our approach on ÆThel, a dataset of type-logical derivations for written
Dutch, where it manages to correctly transcribe raw text sentences into
proofs and terms of the linear λ-calculus with an accuracy of as high as
70%.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>ÆTHEL: Automatically
extracted typelogical derivations for Dutch</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Moortgat Michael, Moot
Richard</div></div><div class="venue"><div> In Proceedings of the 12th
language resources and evaluation conference. May 2020.</div></div>[
<button type="button" class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/2020.lrec-1.647" target="_blank">
paper </a>] [<a
href="https://github.com/konstantinosKokos/Lassy-TLG-Extraction/"
target="_blank"> code </a>] <div class="abstract"><p>We present ÆTHEL, a
semantic compositionality dataset for written Dutch. ÆTHEL consists of
two parts. First, it contains a lexicon of supertags for about 900 000
words in context. The supertags correspond to types of the simply typed
linear lambda-calculus, enhanced with dependency decorations that
capture grammatical roles supplementary to function-argument structures.
On the basis of these types, ÆTHEL further provides 72 192 validated
derivations, presented in four formats: natural-deduction and
sequent-style proofs, linear logic proofnets and the associated programs
(lambda terms) for meaning composition. ÆTHEL’s types and derivations
are obtained by means of an extraction algorithm applied to the
syntactic analyses of LASSY Small, the gold standard corpus of written
Dutch. We discuss the extraction algorithm and show how ‘virtual
elements’ in the original LASSY annotation of unbounded dependencies and
coordination phenomena give rise to higher-order types. We suggest some
example usecases highlighting the benefits of a type-driven approach at
the syntax semantics interface. The following resources are open-sourced
with ÆTHEL: the lexical mappings between words and types, a subset of
the dataset consisting of 7 924 semantic parses, and the Python code
that implements the extraction algorithm.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Deductive Parsing
with an Unbounded Type Lexicon</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Moortgat Michael, Moot
Richard, Tziafas Giorgos</div></div><div class="venue"><div> In
SEMSPACE. August 2019.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://hal-lirmm.ccsd.cnrs.fr/lirmm-02313572" target="_blank">
paper </a>] <div class="abstract"><p>We present a novel deductive
parsing framework for categorial type logics, modeled as the composition
of two components. The first is an attention-based neural supertagger,
which assigns words dependency-decorated, contextually informed linear
types. It requires no predefined type lexicon, instead utilizing the
type syntax to construct types inductively, enabling the use of a richer
and more precise typing environment. The type annotations produced are
used by the second component, a computationally efficient hybrid system
that emulates the inference process of the type logic, iteratively
producing a bottom-up reconstruction of the input’s derivation-proof and
the associated program for compositional meaning assembly. Initial
experiments yield promising results for each of the
components.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Constructive
type-logical supertagging with self-attention networks</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Moortgat Michael, Deoskar
Tejaswini</div></div><div class="venue"><div> In Proceedings of the 4th
workshop on representation learning for NLP (RepL4NLP-2019). August
2019.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/W19-4314" target="_blank"> paper
</a>] [<a
href="https://github.com/konstantinosKokos/Lassy-TLG-Supertagging"
target="_blank"> code </a>] <div class="abstract"><p>We propose a novel
application of self-attention networks towards grammar induction. We
present an attention-based supertagger for a refined type-logical
grammar, trained on constructing types inductively. In addition to
achieving a high overall type accuracy, our model is able to learn the
syntax of the grammar’s type system along with its denotational
semantics. This lifts the closed world assumption commonly made by
lexicalized grammar supertaggers, greatly enhancing its generalization
potential. This is evidenced both by its adequate accuracy over sparse
word types and its ability to correctly construct complex types never
seen during training, which, to the best of our knowledge, was as of yet
unaccomplished.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Towards a 2-multiple
context-free grammar for the 3-dimensional dyck language</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Melkonian
Orestis</div></div><div class="venue"><div> In At the intersection of
language, logic, and information. 2019.</div></div>[ <button
type="button" class="collapsible">abstract</button> ] [<a
href="https://link.springer.com/chapter/10.1007/978-3-662-59620-3_5"
target="_blank"> paper </a>] [<a
href="https://github.com/omelkonian/dyck" target="_blank"> code </a>]
<div class="abstract"><p>We discuss the open problem of parsing the Dyck
language of 3 symbols, D³, using a 2-Multiple Context-Free Grammar. We
attempt to tackle this problem by implementing a number of novel
meta-grammatical techniques and present the associated software packages
we developed.</p></div></li>

</ul>
