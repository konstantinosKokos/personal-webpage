---
 Title: Publications
---
<ul class="paper-list"><li class="paper-item"><div class="papertitle"><div>Diamonds are forever
– theoretical and empirical support for a dependency-enhanced type
logic</div></div><div class="authors"><div>Moortgat Michael, Kogkalidis
Konstantinos, Wijnholds Gijs</div></div><div class="venue"><div> In
Logic and algorithms in computational linguistics 2021.
2022.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://github.com/gijswijnholds/malin_2022" target="_blank"> code
</a>] <div class="abstract"><p>Extended Lambek calculi enlarge the type
language with adjoint pairs of unary modalities. In previous work,
modalities have been used as licensors for controlled forms of
restructuring, reordering and copying. Here, we study a complementary
use of the modalities as dependency features coding for grammatical
roles. The result is a multidimensional type logic simultaneously
inducing dependency and function argument structure on the linguistic
material. We discuss the new perspective on constituent structure
suggested by the dependency-enhanced type logic, and we experimentally
evaluate how well a neural language model like BERT can deal with the
subtle interplay between logical and structural reasoning that this type
logic gives rise to.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Geometry-Aware
Supertagging with Heterogeneous Dynamic Convolutions</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Moortgat
Michael</div></div><div class="venue"><div> In Unpublished.
2022.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://arxiv.org/abs/2203.12235" target="_blank"> paper </a>] [<a
href="https://github.com/konstantinosKokos/dynamic-graph-supertagging"
target="_blank"> code </a>] <div class="abstract"><p>The syntactic
categories of categorial grammar formalisms are structured units made of
smaller, indivisible primitives, bound together by the underlying
grammar’s category formation rules. In the trending approach of
constructive supertagging, neural models are increasingly made aware of
the internal category structure, which in turn enables them to more
reliably predict rare and out-of-vocabulary categories, with significant
implications for grammars previously deemed too complex to find
practical use. In this work, we revisit constructive supertagging from a
graph-theoretic perspective, and propose a framework based on
heterogeneous dynamic graph convolutions aimed at exploiting the
distinctive structure of a supertagger’s output space. We test our
approach on a number of categorial grammar datasets spanning different
languages and grammar formalisms, achieving substantial improvements
over previous state of the art scores.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Discontinuous
Constituency and BERT: A Case Study of Dutch</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Wijnholds
Gijs</div></div><div class="venue"><div> In Findings of the association
for computational linguistics: ACL 2022. May 2022.</div></div>[ <button
type="button" class="collapsible">abstract</button> ] [<a
href="https://aclanthology.org/2022.findings-acl.298" target="_blank">
paper </a>] [<a
href="https://github.com/gijswijnholds/discontinuous-probing"
target="_blank"> code </a>] <div class="abstract"><p>In this paper, we
set out to quantify the syntactic capacity of BERT in the evaluation
regime of non-context free patterns, as occurring in Dutch. We devise a
test suite based on a mildly context-sensitive formalism, from which we
derive grammars that capture the linguistic phenomena of control verb
nesting and verb raising. The grammars, paired with a small lexicon,
provide us with a large collection of naturalistic utterances, annotated
with verb-subject pairings, that serve as the evaluation test bed for an
attention-based span selection probe. Our results, backed by extensive
analysis, suggest that the models investigated fail in the implicit
acquisition of the dependencies examined.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>A Logic-Based
Framework for Natural Language Inference in Dutch</div></div><div
class="authors"><div>Abzianidze Lasha, Kogkalidis
Konstantinos</div></div><div class="venue"><div> In Journal of
computational linguistics in the Netherlands. 2022.</div></div>[ <button
type="button" class="collapsible">abstract</button> ] [<a
href="https://www.clinjournal.org/index.php/clinj/article/view/120"
target="_blank"> paper </a>] [<a href="https://git.io/JzdGd"
target="_blank"> code </a>] <div class="abstract"><p>We present a
framework for deriving inference relations between Dutch sentence pairs.
The proposed framework relies on logic-based reasoning to produce
inspectable proofs leading up to inference labels; its judgements are
therefore transparent and formally verifiable. At its core, the system
is powered by two λ-calculi, used as syntactic and semantic theories,
respectively. Sentences are first converted to syntactic proofs and
terms of the linear λ-calculus using a choice of two parsers: an
Alpino-based pipeline, or Neural Proof Nets. The syntactic terms are
then converted to semantic terms of the simply typed λ-calculus, via a
set of hand designed type- and term-level transformations. Pairs of
semantic terms are then fed to an automated theorem prover for natural
logic which reasons with them while using the lexical relations found in
the Open Dutch WordNet. We evaluate the reasoning pipeline on the
recently created Dutch natural language inference dataset, and achieve
promising results, remaining only within a 1.1–3.2% performance margin
to strong neural baselines. To the best of our knowledge, the reasoning
pipeline is the first logic-based system for Dutch.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Fighting the
COVID-19 Infodemic with a Holistic BERT Ensemble</div></div><div
class="authors"><div>Tziafas Georgios, Kogkalidis Konstantinos, Caselli
Tommaso</div></div><div class="venue"><div> In Proceedings of the fourth
workshop on nlp for internet freedom: Censorship, disinformation, and
propaganda. June 2021.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/2021.nlp4if-1.18"
target="_blank"> paper </a>] [<a
href="https://github.com/gtziafas/nlp4ifchallenge" target="_blank"> code
</a>] <div class="abstract"><p>This paper describes the TOKOFOU system,
an ensemble model for misinformation detection tasks based on six
different transformer-based pre-trained encoders, implemented in the
context of the COVID-19 Infodemic Shared Task for English. We fine tune
each model on each of the task’s questions and aggregate their
prediction scores using a majority voting approach. TOKOFOU obtains an
overall F1 score of 89.7%, ranking first.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Neural Proof
Nets</div></div><div class="authors"><div>Kogkalidis Konstantinos,
Moortgat Michael, Moot Richard</div></div><div class="venue"><div> In
Proceedings of the 24th conference on computational natural language
learning. November 2020.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/2020.conll-1.3" target="_blank">
paper </a>] [<a
href="https://github.com/konstantinosKokos/neural-proof-nets"
target="_blank"> code </a>] <div class="abstract"><p>Linear logic and
the linear λ-calculus have a long standing tradition in the study of
natural language form and meaning. Among the proof calculi of linear
logic, proof nets are of particular interest, offering an attractive
geometric representation of derivations that is unburdened by the
bureaucratic complications of conventional prooftheoretic formats.
Building on recent advances in set-theoretic learning, we propose a
neural variant of proof nets based on Sinkhorn networks, which allows us
to translate parsing as the problem of extracting syntactic primitives
and permuting them into alignment. Our methodology induces a
batch-efficient, end-to-end differentiable architecture that actualizes
a formally grounded yet highly efficient neuro-symbolic parser. We test
our approach on ÆThel, a dataset of type-logical derivations for written
Dutch, where it manages to correctly transcribe raw text sentences into
proofs and terms of the linear λ-calculus with an accuracy of as high as
70%.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>ÆTHEL: Automatically
Extracted Typelogical Derivations for Dutch</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Moortgat Michael, Moot
Richard</div></div><div class="venue"><div> In Proceedings of the 12th
language resources and evaluation conference. May 2020.</div></div>[
<button type="button" class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/2020.lrec-1.647" target="_blank">
paper </a>] [<a
href="https://github.com/konstantinosKokos/Lassy-TLG-Extraction/"
target="_blank"> code </a>] <div class="abstract"><p>We present ÆTHEL, a
semantic compositionality dataset for written Dutch. ÆTHEL consists of
two parts. First, it contains a lexicon of supertags for about 900 000
words in context. The supertags correspond to types of the simply typed
linear lambda-calculus, enhanced with dependency decorations that
capture grammatical roles supplementary to function-argument structures.
On the basis of these types, ÆTHEL further provides 72 192 validated
derivations, presented in four formats: natural-deduction and
sequent-style proofs, linear logic proofnets and the associated programs
(lambda terms) for meaning composition. ÆTHEL’s types and derivations
are obtained by means of an extraction algorithm applied to the
syntactic analyses of LASSY Small, the gold standard corpus of written
Dutch. We discuss the extraction algorithm and show how ‘virtual
elements’ in the original LASSY annotation of unbounded dependencies and
coordination phenomena give rise to higher-order types. We suggest some
example usecases highlighting the benefits of a type-driven approach at
the syntax semantics interface. The following resources are open-sourced
with ÆTHEL: the lexical mappings between words and types, a subset of
the dataset consisting of 7 924 semantic parses, and the Python code
that implements the extraction algorithm.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Deductive Parsing
with an Unbounded Type Lexicon</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Moortgat Michael, Moot
Richard, Tziafas Giorgos</div></div><div class="venue"><div> In
SEMSPACE. August 2019.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://hal-lirmm.ccsd.cnrs.fr/lirmm-02313572" target="_blank">
paper </a>] <div class="abstract"><p>We present a novel deductive
parsing framework for categorial type logics, modeled as the composition
of two components. The first is an attention-based neural supertagger,
which assigns words dependency-decorated, contextually informed linear
types. It requires no predefined type lexicon, instead utilizing the
type syntax to construct types inductively, enabling the use of a richer
and more precise typing environment. The type annotations produced are
used by the second component, a computationally efficient hybrid system
that emulates the inference process of the type logic, iteratively
producing a bottom-up reconstruction of the input’s derivation-proof and
the associated program for compositional meaning assembly. Initial
experiments yield promising results for each of the
components.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Constructive
Type-Logical Supertagging with Self-Attention Networks</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Moortgat Michael, Deoskar
Tejaswini</div></div><div class="venue"><div> In Proceedings of the 4th
workshop on representation learning for NLP (RepL4NLP-2019). August
2019.</div></div>[ <button type="button"
class="collapsible">abstract</button> ] [<a
href="https://www.aclweb.org/anthology/W19-4314" target="_blank"> paper
</a>] [<a
href="https://github.com/konstantinosKokos/Lassy-TLG-Supertagging"
target="_blank"> code </a>] <div class="abstract"><p>We propose a novel
application of self-attention networks towards grammar induction. We
present an attention-based supertagger for a refined type-logical
grammar, trained on constructing types inductively. In addition to
achieving a high overall type accuracy, our model is able to learn the
syntax of the grammar’s type system along with its denotational
semantics. This lifts the closed world assumption commonly made by
lexicalized grammar supertaggers, greatly enhancing its generalization
potential. This is evidenced both by its adequate accuracy over sparse
word types and its ability to correctly construct complex types never
seen during training, which, to the best of our knowledge, was as of yet
unaccomplished.</p></div></li>

<li class="paper-item"><div class="papertitle"><div>Towards a 2-Multiple
Context-Free Grammar for the 3-Dimensional Dyck Language</div></div><div
class="authors"><div>Kogkalidis Konstantinos, Melkonian
Orestis</div></div><div class="venue"><div> In At the intersection of
language, logic, and information. 2019.</div></div>[ <button
type="button" class="collapsible">abstract</button> ] [<a
href="https://link.springer.com/chapter/10.1007/978-3-662-59620-3_5"
target="_blank"> paper </a>] [<a
href="https://github.com/omelkonian/dyck" target="_blank"> code </a>]
<div class="abstract"><p>We discuss the open problem of parsing the Dyck
language of 3 symbols, D³, using a 2-Multiple Context-Free Grammar. We
attempt to tackle this problem by implementing a number of novel
meta-grammatical techniques and present the associated software packages
we developed.</p></div></li>

</ul>
