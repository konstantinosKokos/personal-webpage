@inproceedings{https://doi.org/10.48550/arxiv.2203.12235,
  doi = {10.48550/ARXIV.2203.12235},
  url = {https://arxiv.org/abs/2203.12235},
  author = {Kogkalidis, Konstantinos and Moortgat, Michael},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions},
  booktitle={Unpublished},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  note={https://github.com/konstantinosKokos/dynamic-graph-supertagging},
  abstract={The syntactic categories of categorial grammar formalisms are structured units made of smaller, indivisible primitives, bound together by the underlying grammar's category formation rules. In the trending approach of constructive supertagging, neural models are increasingly made aware of the internal category structure, which in turn enables them to more reliably predict rare and out-of-vocabulary categories, with significant implications for grammars previously deemed too complex to find practical use. In this work, we revisit constructive supertagging from a graph-theoretic perspective, and propose a framework based on heterogeneous dynamic graph convolutions aimed at exploiting the distinctive structure of a supertagger's output space. We test our approach on a number of categorial grammar datasets spanning different languages and grammar formalisms, achieving substantial improvements over previous state of the art scores.}
}
@InProceedings{https://doi.org/10.48550/arxiv.2203.01063,
  doi = {10.48550/ARXIV.2203.01063},
  url = {https://arxiv.org/abs/2203.01063},
  author = {Kogkalidis, Konstantinos and Wijnholds, Gijs},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Discontinuous Constituency and {BERT}: A Case Study of Dutch},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  booktitle={Findings of {ACL} 2022 (to appear)},
  note={https://github.com/gijswijnholds/discontinuous-probing},
  abstract={In this paper, we set out to quantify the syntactic capacity of {BERT} in the evaluation regime of non-context free patterns, as occurring in {D}utch. We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising. The grammars, paired with a small lexicon, provide us with a large collection of naturalistic utterances, annotated with verb-subject pairings, that serve as the evaluation test bed for an attention-based span selection probe. Our results, backed by extensive analysis, suggest that the models investigated fail in the implicit acquisition of the dependencies examined.}
}
@inproceedings{Abzianidze_Kogkalidis_2022,
  title= "A {L}ogic-{B}ased {F}ramework for {N}atural {L}anguage {I}nference in {D}utch",
  volume="11",
  url="https://www.clinjournal.org/index.php/clinj/article/view/120",
  abstract="We present a framework for deriving inference relations between Dutch sentence pairs. The proposed framework relies on logic-based reasoning to produce inspectable proofs leading up to inference labels; its judgements are therefore transparent and formally verifiable. At its core, the system is powered by two λ-calculi, used as syntactic and semantic theories, respectively. Sentences are first converted to syntactic proofs and terms of the linear λ-calculus using a choice of two parsers: an Alpino-based pipeline, or Neural Proof Nets. The syntactic terms are then converted to semantic terms of the simply typed λ-calculus, via a set of hand designed type- and term-level transformations. Pairs of semantic terms are then fed to an automated theorem prover for natural logic which reasons with them while using the lexical relations found in the Open Dutch WordNet. We evaluate the reasoning pipeline on the recently created Dutch natural language inference dataset, and achieve promising results, remaining only within a 1.1–3.2\% performance margin to strong neural baselines. To the best of our knowledge, the reasoning pipeline is the first logic-based system for Dutch.",
  booktitle="Journal of Computational Linguistics in the {N}etherlands",
  note="https://git.io/JzdGd",
  author="Abzianidze, Lasha and Kogkalidis, Konstantinos",
  year="2022",
  month="Feb.",
  pages="35–58"
}
@inproceedings{tziafas-etal-2021-fighting,
    title = "Fighting the {COVID}-19 Infodemic with a Holistic {BERT} Ensemble",
    author = "Tziafas, Georgios  and
      Kogkalidis, Konstantinos  and
      Caselli, Tommaso",
    booktitle = "Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.nlp4if-1.18",
    doi = "10.18653/v1/2021.nlp4if-1.18",
    pages = "119--124",
    abstract = "This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task{'}s questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7{\%}, ranking first.",
    note = {https://github.com/gtziafas/nlp4ifchallenge}
}
@inproceedings{kogkalidis-etal-2020-neural,
    title = "Neural Proof Nets",
    author = "Kogkalidis, Konstantinos  and
      Moortgat, Michael  and
      Moot, Richard",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.conll-1.3",
    pages = "26--40",
    abstract = "Linear logic and the linear $λ$-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on {\AE}Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear $λ$-calculus with an accuracy of as high as 70{\%}.",
    note = {https://github.com/konstantinosKokos/neural-proof-nets}
}
@inproceedings{kogkalidis-etal-2020-aethel,
    title = "{{\AE}THEL}: Automatically Extracted Typelogical Derivations for {D}utch",
    author = "Kogkalidis, Konstantinos  and
      Moortgat, Michael  and
      Moot, Richard",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.647",
    pages = "5257--5266",
    abstract = "We present {\AE}THEL, a semantic compositionality dataset for written Dutch. {\AE}THEL consists of two parts. First, it contains a lexicon of supertags for about 900 000 words in context. The supertags correspond to types of the simply typed linear lambda-calculus, enhanced with dependency decorations that capture grammatical roles supplementary to function-argument structures. On the basis of these types, {\AE}THEL further provides 72 192 validated derivations, presented in four formats: natural-deduction and sequent-style proofs, linear logic proofnets and the associated programs (lambda terms) for meaning composition. {\AE}THEL{'}s types and derivations are obtained by means of an extraction algorithm applied to the syntactic analyses of LASSY Small, the gold standard corpus of written Dutch. We discuss the extraction algorithm and show how {`}virtual elements{'} in the original LASSY annotation of unbounded dependencies and coordination phenomena give rise to higher-order types. We suggest some example usecases highlighting the benefits of a type-driven approach at the syntax semantics interface. The following resources are open-sourced with {\AE}THEL: the lexical mappings between words and types, a subset of the dataset consisting of 7 924 semantic parses, and the Python code that implements the extraction algorithm.",
    language = "English",
    ISBN = "979-10-95546-34-4",
    note = "https://github.com/konstantinosKokos/Lassy-TLG-Extraction/"
}
@inproceedings{kogkalidis:lirmm-02313572,
  TITLE = {{Deductive Parsing with an Unbounded Type Lexicon}},
  AUTHOR = {Kogkalidis, Konstantinos and Moortgat, Michael and Moot, Richard and Tziafas, Giorgos},
  URL = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-02313572},
  BOOKTITLE = {{SEMSPACE}},
  ADDRESS = {Riga, Latvia},
  YEAR = {2019},
  MONTH = Aug,
  PDF = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-02313572/file/SEMSPACE2019_paper_4.pdf},
  HAL_ID = {lirmm-02313572},
  HAL_VERSION = {v1},
  abstract={We present a novel deductive parsing framework for categorial type logics, modeled as the composition of two components. The first is an attention-based neural supertagger, which assigns words dependency-decorated, contextually informed linear types. It requires no predefined type lexicon, instead utilizing the type syntax to construct types inductively, enabling the use of a richer and more precise typing environment. The type annotations produced are used by the second component, a computationally efficient hybrid system that emulates the inference process of the type logic, iteratively producing a bottom-up reconstruction of the input's derivation-proof and the associated program for compositional meaning assembly. Initial experiments yield promising results for each of the components. }
}
@inproceedings{kogkalidis-etal-2019-constructive,
    title = "Constructive Type-Logical Supertagging With Self-Attention Networks",
    author = "Kogkalidis, Konstantinos  and
      Moortgat, Michael  and
      Deoskar, Tejaswini",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for {NLP} ({RepL4NLP-2019})",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4314",
    doi = "10.18653/v1/W19-4314",
    pages = "113--123",
    abstract = "We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammar{'}s type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.",
    note = "https://github.com/konstantinosKokos/Lassy-TLG-Supertagging"
}
@InProceedings{10.1007/978-3-662-59620-3_5,
  author="Kogkalidis, Konstantinos
  and Melkonian, Orestis",
  editor="Sikos, Jennifer
  and Pacuit, Eric",
  title="Towards a 2-Multiple Context-Free Grammar for the 3-Dimensional Dyck Language",
  booktitle="At the Intersection of Language, Logic, and Information",
  year="2019",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="79--92",
  abstract="We discuss the open problem of parsing the Dyck language of 3 symbols, $D^3$, using a 2-Multiple Context-Free Grammar. We attempt to tackle this problem by implementing a number of novel meta-grammatical techniques and present the associated software packages we developed.",
  isbn="978-3-662-59620-3",
  url="https://link.springer.com/chapter/10.1007/978-3-662-59620-3_5",
  note="https://github.com/omelkonian/dyck"
}
